<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Forecasting on Len Kiefer</title>
    <link>/tags/forecasting/</link>
    <description>Recent content in Forecasting on Len Kiefer</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2015-2018. All rights reserved.</copyright>
    <lastBuildDate>Mon, 17 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/forecasting/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Forecasting with logs</title>
      <link>/2020/02/17/forecasting-with-logs/</link>
      <pubDate>Mon, 17 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/17/forecasting-with-logs/</guid>
      <description>As an economist and all-around friend of strictly positive numbers I often use the log function. The natural logarithm of course, need I specify it? Apparently in certain spreadsheet software you do.
In this note I just wanted to write down a couple of observations about how to generate mean or median forecasts of a variable \(y\) given the model is fit in \(log(y)\). Of course, I am going to borrow heavily from Rob Hyndman’s blog, where he coverse this.</description>
    </item>
    
    <item>
      <title>Lower Mortgage Rates Bolster the Housing Market</title>
      <link>/2019/11/02/lower-mortgage-rates-bolster-the-housing-market/</link>
      <pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/02/lower-mortgage-rates-bolster-the-housing-market/</guid>
      <description>Mortgage interest rates have moved about a percentage point lower from where they were a year ago. The housing market seems to have responded favorably.
On my way into D.C. the other day to do some business, I joined a Twitter exchange originally between [at]Graykimbrough and Adam Ozimek, [at]ModeledBehavior about the effects of Federal Reserve interest policy on the housing market.
Seems unlikely housing market was slowed by trade war.</description>
    </item>
    
    <item>
      <title>Forecasts from a bivariate VECM conditional on one of the variables</title>
      <link>/2019/10/24/forecasts-from-a-bivariate-vecm-conditional-on-one-of-the-variables/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/24/forecasts-from-a-bivariate-vecm-conditional-on-one-of-the-variables/</guid>
      <description>This post is for me and future me, though if you get something out of that, that’s great too. Here I will jot down some notes on something I’ve been thinking about.
Because reasons, I have been interested in Vector Error Correction Models (VECM). I’ve been thinking of the case where you estimate an error correction model, and have available external forecasts for one of the variables. How can you easily construct the conditional forecasts for the VECM in R?</description>
    </item>
    
    <item>
      <title>Forecasting is still hard</title>
      <link>/2018/12/19/forecasting-is-still-hard/</link>
      <pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/19/forecasting-is-still-hard/</guid>
      <description>It’s the time of the year where everybody is dusting off their crystal balls and peering into the future. There’s even still time to send out your “Winter is Coming” newsletter.
Let’s take a step back and look at how forecasts of U.S. macro variables have evolved. Is forecasting still hard?
Last year we looked at historical forecasts of economic conditions in the post forecasting is hard. Let’s update it.</description>
    </item>
    
    <item>
      <title>Vulnerable Housing</title>
      <link>/2018/12/12/vulnerable-housing/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/12/vulnerable-housing/</guid>
      <description>My recent economic and housing market talks see for example here have been titled: “Will the U.S. housing market get back on track in 2019?”. My general conclusion has been cautiously optimistic. There is enough strength in the broader economy and enough of a tailwind from demographic forces to push the U.S. housing market to modest growth next year.
I still think that’s true, but as I have said in my talks, risks are weighted to the downside.</description>
    </item>
    
    <item>
      <title>Housing Market Outlook</title>
      <link>/2018/11/29/housing-market-outlook/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/29/housing-market-outlook/</guid>
      <description>The year is winding down, and folks are starting to think about next year. With lots of folks reviewing strategic plans and whatnot, there’s increased demand for me to talk about my 2019 economic outlook.
Over on LinkedIn I posted a summary of my most recent chartbook: Will the housing market get back on track in 2019?.
Do check it out.
SlidecraftFor these slides I used a mixture of R and Excel.</description>
    </item>
    
    <item>
      <title>Kalman Filter for a dynamic linear model in R</title>
      <link>/2018/06/10/kalman-filter-for-a-dynamic-linear-model-in-r/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/10/kalman-filter-for-a-dynamic-linear-model-in-r/</guid>
      <description>As an economist with a background in econometrics and forecasting I recognize that predictions are often (usually?) an exercise in futility. Forecasting, after all, is hard. While non-economists have great fun pointing this futility out, many critics miss out on why it’s so hard.
There are at least two reasons why forecasting is hard. The first, the unknown future, is pretty well understood. Empirical regularities with much forecasting power in the social sciences are hard to come by and are rarely stable.</description>
    </item>
    
    <item>
      <title>Forecasting Game</title>
      <link>/2018/03/12/forecasting-game/</link>
      <pubDate>Mon, 12 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/12/forecasting-game/</guid>
      <description>LET’S PICK BACK UP where we left off and think about communicating forecast results. To help guide our thinking, let’s set up a little game.
Basic setupLike last time we’re going to focus on a situation where a forecaster observes some information about the world and makes an announcement about a future binary outcome. A decision maker observes the forecaster’s announcement and takes a binary action. Then the outcome is realized and the forecaster receives a payoff.</description>
    </item>
    
    <item>
      <title>Forecasting and deciding binary outcomes under asymmetric information</title>
      <link>/2018/03/05/forecasting-and-deciding-binary-outcomes-under-asymmetric-information/</link>
      <pubDate>Mon, 05 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/05/forecasting-and-deciding-binary-outcomes-under-asymmetric-information/</guid>
      <description>LAST WEEK IN THE WALL STREET JOURNAL an article LINK talked about how pundits can strategically make probabilistic forecasts. It seems 40% is a sort of magic number, where it’s high enough that if the event comes true you can claim credit as a forecaster, but if it doesn’t happen, you still gave it less than 50/50 odds.
Since I’m often asked to make forecasts I’m interested in this problem.</description>
    </item>
    
    <item>
      <title>A closer look at forecasting recessions with dynamic model averaging</title>
      <link>/2017/10/28/dma-part-2/</link>
      <pubDate>Sat, 28 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/28/dma-part-2/</guid>
      <description>BACK WE GO INTO THE VASTY DEEP. LAST TIME we introduced the idea of using dynamic model averaging to forecast recessions. I was so excited about the new approach that I didn’t take the time to break down what was going on with it. In this post we’ll look more closely at what’s happening with the dma packaged when we try to forecast recessions.
Per usual we’ll do it with R and I’ll include code so you can follow along.</description>
    </item>
    
    <item>
      <title>Forecasting recessions with dynamic model averaging</title>
      <link>/2017/10/26/predicting-recessions-with-dynamic-model-averaging/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/26/predicting-recessions-with-dynamic-model-averaging/</guid>
      <description>HERE THE LITERATURE IS VASTY DEEP. In this post we’ll dip our toes, every so slightly, into the dark waters of macroeconometric forecasting. I’ve been studying some techniques and want to try them out. I’m still at the learning and exploring stage, but let’s do it together.
In this post we’ll conduct an exercise in forecasting U.S. recessions using several approaches. Per usual we’ll do it with R and I’ll include code so you can follow along.</description>
    </item>
    
    <item>
      <title>Forecasting is hard (work)</title>
      <link>/2017/08/27/forecast/</link>
      <pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/27/forecast/</guid>
      <description>IN THIS POST WE WILL STUDY FORECASTS OF US ECONOMIC CONDITIONS.
Niels Bohr quipped:
Prediction is very difficult, especially if it’s about the future.
I’m a macroeconomist by training, and my day job sometimes requires me to forecast the future so I can relate. Predicting the future can be quite difficult.
In this post, we’ll analyze forecasts of economic conditions from professional forecasters using R to wrangle the data and construct plots.</description>
    </item>
    
  </channel>
</rss>